{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef49dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07c015a",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf0ed7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Sentence #           Word  POS    Tag\n",
      "0   Sentence: 1      Thousands  NNS      O\n",
      "1           NaN             of   IN      O\n",
      "2           NaN  demonstrators  NNS      O\n",
      "3           NaN           have  VBP      O\n",
      "4           NaN        marched  VBN      O\n",
      "5           NaN        through   IN      O\n",
      "6           NaN         London  NNP  B-geo\n",
      "7           NaN             to   TO      O\n",
      "8           NaN        protest   VB      O\n",
      "9           NaN            the   DT      O\n",
      "10          NaN            war   NN      O\n",
      "11          NaN             in   IN      O\n",
      "12          NaN           Iraq  NNP  B-geo\n",
      "13          NaN            and   CC      O\n",
      "14          NaN         demand   VB      O\n",
      "15          NaN            the   DT      O\n",
      "16          NaN     withdrawal   NN      O\n",
      "17          NaN             of   IN      O\n",
      "18          NaN        British   JJ  B-gpe\n",
      "19          NaN         troops  NNS      O\n",
      "20          NaN           from   IN      O\n",
      "21          NaN           that   DT      O\n",
      "22          NaN        country   NN      O\n",
      "23          NaN              .    .      O\n",
      "24  Sentence: 2       Families  NNS      O\n",
      "25          NaN             of   IN      O\n",
      "26          NaN       soldiers  NNS      O\n",
      "27          NaN         killed  VBN      O\n",
      "28          NaN             in   IN      O\n",
      "29          NaN            the   DT      O\n",
      "30          NaN       conflict   NN      O\n",
      "31          NaN         joined  VBD      O\n",
      "32          NaN            the   DT      O\n",
      "33          NaN     protesters  NNS      O\n",
      "34          NaN            who   WP      O\n",
      "35          NaN        carried  VBD      O\n",
      "36          NaN        banners  NNS      O\n",
      "37          NaN           with   IN      O\n",
      "38          NaN           such   JJ      O\n",
      "39          NaN        slogans  NNS      O\n",
      "40          NaN             as   IN      O\n",
      "41          NaN              \"   ``      O\n",
      "42          NaN           Bush  NNP  B-per\n",
      "43          NaN         Number   NN      O\n",
      "44          NaN            One   CD      O\n",
      "45          NaN      Terrorist   NN      O\n",
      "46          NaN              \"   ``      O\n",
      "47          NaN            and   CC      O\n",
      "48          NaN              \"   ``      O\n",
      "49          NaN           Stop   VB      O\n",
      "Sentence #    1000616\n",
      "Word               10\n",
      "POS                 0\n",
      "Tag                 0\n",
      "dtype: int64\n",
      "Tag\n",
      "O        887908\n",
      "B-geo     37644\n",
      "B-tim     20333\n",
      "B-org     20143\n",
      "I-per     17251\n",
      "B-per     16990\n",
      "I-org     16784\n",
      "B-gpe     15870\n",
      "I-geo      7414\n",
      "I-tim      6528\n",
      "B-art       402\n",
      "B-eve       308\n",
      "I-art       297\n",
      "I-eve       253\n",
      "B-nat       201\n",
      "I-gpe       198\n",
      "I-nat        51\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(r\".\\NER dataset.csv\", encoding='ISO-8859-1')\n",
    "\n",
    "print(dataset.head(50))\n",
    "print(dataset.isna().sum())\n",
    "print(dataset['Tag'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f4efed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling Na\n",
    "dataset = dataset.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "933f9946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping words and tags by sentence\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "grouped = dataset.groupby('Sentence #')\n",
    "\n",
    "for _, group in grouped:\n",
    "    words = list(group['Word'])\n",
    "    tags = list(group['Tag'])\n",
    "    sentences.append(words)\n",
    "    labels.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d6123fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47959\n",
      "47959\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e317a255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceVocab:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.max_sent_len = 0\n",
    "        self.n_words = 0\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "\n",
    "        # Special tokens\n",
    "        self.PAD_TOKEN = \"<PAD>\"\n",
    "        self.UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "        self.addWord(self.PAD_TOKEN) # index 0\n",
    "        self.addWord(self.UNK_TOKEN) # index 1\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        if len(sentence) > self.max_sent_len:\n",
    "            self.max_sent_len = len(sentence)\n",
    "\n",
    "        for word in sentence:\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.n_words\n",
    "            self.idx2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "\n",
    "class TagVocab:\n",
    "    def __init__(self):\n",
    "        self.n_tags = 0\n",
    "        self.tag2idx = {}\n",
    "        self.idx2tag = {}\n",
    "\n",
    "    def addTags(self, tags):\n",
    "        for tag in tags:\n",
    "            self.addTag(tag)\n",
    "\n",
    "    def addTag(self, tag):\n",
    "        if tag not in self.tag2idx:\n",
    "            self.tag2idx[tag] = self.n_tags\n",
    "            self.idx2tag[self.n_tags] = tag\n",
    "            self.n_tags += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "71cb35c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vocab = SentenceVocab()\n",
    "tag_vocab = TagVocab()\n",
    "\n",
    "for word, tag_seq in zip(sentences, labels):\n",
    "    word_vocab.addSentence(word)\n",
    "    tag_vocab.addTags(tag_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c23a156",
   "metadata": {},
   "source": [
    "Creating Dataloader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "96fd0bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, tags, word_vocab, tag_vocab):\n",
    "        \"\"\"\n",
    "        sentences: list of list of words\n",
    "        tags: list of list of tags\n",
    "        word_vocab: Vocab object for words\n",
    "        tag_vocab: Vocab object for tags\n",
    "        \"\"\"\n",
    "\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.word_vocab = word_vocab\n",
    "        self.tag_vocab = tag_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.tags[idx]\n",
    "        \n",
    "def pad_sequence(sequences, pad_value):\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    return [seq + [pad_value]*(max_len- len(seq)) for seq in sequences]\n",
    "\n",
    "def create_collate_fn(word_vocab:SentenceVocab, tag_vocab:TagVocab):\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"\n",
    "        batch: list of (sentence, tag) pairs\n",
    "        \"\"\"\n",
    "\n",
    "        sentences, tags = zip(*batch)\n",
    "\n",
    "        # convert words to IDs\n",
    "        word_ids = [[word_vocab.word2idx.get(w, word_vocab.word2idx['<UNK>']) for w in sent] for sent in sentences]\n",
    "        tag_ids = [[tag_vocab.tag2idx.get(t) for t in tag_seq] for tag_seq in tags]\n",
    "\n",
    "        # pad sequences\n",
    "        word_ids = pad_sequence(word_ids, word_vocab.word2idx[\"<PAD>\"])\n",
    "        tag_ids = pad_sequence(tag_ids, tag_vocab.tag2idx[\"O\"]) # default pad tag = \"O\" (outside)\n",
    "        \n",
    "        # Create attention mask (1 for real tokens, 0 for pad)\n",
    "        attention_masks = [[1 if token != word_vocab.word2idx[\"<PAD>\"] else 0 for token in seq] for seq in word_ids]\n",
    "\n",
    "        return (\n",
    "            torch.tensor(word_ids, dtype=torch.long),\n",
    "            torch.tensor(tag_ids, dtype=torch.long),\n",
    "            torch.tensor(attention_masks, dtype=torch.long)\n",
    "        )\n",
    "    \n",
    "    return collate_fn\n",
    "\n",
    "def get_device():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78926d19",
   "metadata": {},
   "source": [
    "Creating Encoder Only Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bbade1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads=8, masked=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert hidden_size % num_heads == 0, \"hidden_size must be divisible by num_heads\"\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.masked = masked\n",
    "\n",
    "        # Defining query, key and value weights\n",
    "        self.query_W = nn.Linear(self.hidden_size, hidden_size)\n",
    "        self.key_W = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.value_W = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        # Output Layer\n",
    "        self.output_layer = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        key = self.key_W(x)\n",
    "        query = self.query_W(x)\n",
    "        value = self.value_W(x)\n",
    "\n",
    "        Q = self.split_head(query)\n",
    "        K = self.split_head(key)\n",
    "        V = self.split_head(value)\n",
    "\n",
    "        # Calculate Attention Scores.\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim**0.5)\n",
    "\n",
    "        if self.masked and mask is not None:\n",
    "            scores = scores.masked_fill(mask==0, float(\"-1e20\"))\n",
    "\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        # Concatenate heads\n",
    "        # Note: view() requires the underlying tensor to be contiguous. If you try to call view() on a non-contiguous tensor, PyTorch will raise a runtime error.\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.num_heads * self.head_dim)\n",
    "\n",
    "        # Final output projection\n",
    "        output = self.output_layer(attn_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def split_head(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        return x.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embd_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(embd_size, heads)\n",
    "\n",
    "        # Normalization layers\n",
    "        self.norm_1 = nn.LayerNorm(embd_size)\n",
    "        self.norm_2 = nn.LayerNorm(embd_size)\n",
    "\n",
    "        # Feedforward Network\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(embd_size, forward_expansion*embd_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion*embd_size, embd_size)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "    \n",
    "        # implementing pre normalization\n",
    "        attn_out = self.attention(self.norm_1(x), mask)\n",
    "\n",
    "        x = x + self.dropout(attn_out)\n",
    "\n",
    "        ff_out = self.feedforward(self.norm_2(x))\n",
    "        out = x + self.dropout(ff_out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embd_dim, heads, forward_expansion, dropout, vocab_size, max_sent_len):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embd_dim = embd_dim\n",
    "        self.heads = heads\n",
    "\n",
    "        self.word_embd = nn.Embedding(vocab_size, embd_dim)\n",
    "        self.pos_embd = nn.Embedding(max_sent_len, embd_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embd_dim,\n",
    "                    heads,\n",
    "                    dropout,\n",
    "                    forward_expansion\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(x.device)\n",
    "\n",
    "        out = self.dropout(self.word_embd(x) + self.pos_embd(positions))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, mask) # Key, Query, Value all will be same\n",
    "\n",
    "        return out\n",
    "    \n",
    "class NERNeuralNetwork(nn.Module):\n",
    "    def __init__(self, embd_dim, heads, forward_expansion, dropout, vocab_size, max_sent_len, total_tags):\n",
    "        super(NERNeuralNetwork, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(embd_dim=embd_dim, heads=heads, forward_expansion=forward_expansion, dropout=dropout, vocab_size=vocab_size, max_sent_len=max_sent_len)\n",
    "        self.linear = nn.Linear(embd_dim, total_tags)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        x = self.encoder(x, attn_mask)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b0fcfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Training Loop\n",
    "import os\n",
    "import time\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "def train(dataloader, ner_model, n_epochs, print_every, learning_rate):\n",
    "    start = time.time()\n",
    "\n",
    "    ner_model = ner_model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(ner_model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    # Initialize epoch progress bar\n",
    "    epoch_bar = tqdm(range(1, n_epochs + 1), desc=\"Training\", unit=\"epoch\")\n",
    "\n",
    "    for epoch in epoch_bar:\n",
    "        \n",
    "        ner_model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        # Initialize batch progress bar for current epoch\n",
    "        batch_bar = tqdm(dataloader, desc=f\"Epoch {epoch}/{n_epochs}\", leave=False)\n",
    "\n",
    "        for inputs, labels, attn_mask in batch_bar:\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            attn_mask = attn_mask.to(device)\n",
    "\n",
    "            # Get predictions from the model\n",
    "            output = ner_model(inputs, attn_mask)\n",
    "\n",
    "            # Empty gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = criterion(output.view(-1, output.shape[-1]), labels.view(-1))\n",
    "\n",
    "            # calculate gradients w.r.t loss function.\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights.\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Update batch progress bar\n",
    "            batch_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        # Calculate average loss for this epoch\n",
    "        avg_loss = total_loss / num_batches\n",
    "        \n",
    "        # Update epoch progress bar\n",
    "        epoch_bar.set_postfix({'Avg Loss': f'{avg_loss:.4f}'})\n",
    "\n",
    "        # Print summary every few epochs\n",
    "        if epoch % print_every == 0:\n",
    "            tqdm.write(f\"Epoch {epoch}/{n_epochs} - Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(f\"Training finished in {time.time() - start:.2f}s\")\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    ner_path = os.path.join(\"./\", \"ner_model.pth\")\n",
    "    torch.save(ner_model.state_dict(), ner_path)\n",
    "    print(f\"Model saved to {ner_path}\")\n",
    "\n",
    "def evaluate(dataloader, ner_model):\n",
    "    # set the model to evaluate mode.\n",
    "    ner_model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, attn_mask in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            attn_mask = attn_mask.to(device)\n",
    "\n",
    "            outputs = ner_model(inputs, attn_mask)\n",
    "\n",
    "            loss = criterion(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy (optional)\n",
    "            predictions = outputs.argmax(dim=-1)  # pick class with max logit\n",
    "            mask = labels != 0  # ignore PAD tokens (if 0 is PAD)\n",
    "            correct = (predictions == labels) & mask\n",
    "            total_correct += correct.sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_tokens if total_tokens > 0 else 0.0\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "22e5c872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 10/50 [03:08<12:33, 18.84s/epoch, Avg Loss=0.5032]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 - Avg Loss: 0.5032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 20/50 [06:09<09:03, 18.10s/epoch, Avg Loss=0.3594]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 - Avg Loss: 0.3594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 30/50 [09:10<06:04, 18.22s/epoch, Avg Loss=0.2766]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50 - Avg Loss: 0.2766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 40/50 [12:18<03:09, 18.94s/epoch, Avg Loss=0.2177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50 - Avg Loss: 0.2177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [15:28<00:00, 18.56s/epoch, Avg Loss=0.1724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Avg Loss: 0.1724\n",
      "Training finished in 928.10s\n",
      "Saving model...\n",
      "Model saved to ./ner_model.pth\n",
      "Evaluating the model.\n",
      "Validation Loss: 1.1910, Accuracy: 0.8502\n"
     ]
    }
   ],
   "source": [
    "dataset = NERDataset(sentences=sentences, tags=labels, word_vocab=word_vocab, tag_vocab=tag_vocab)\n",
    "\n",
    "# Define split sizes (e.g., 80% train, 20% test)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "my_collate_fn = create_collate_fn(word_vocab, tag_vocab)\n",
    "\n",
    "# Random split\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=my_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=my_collate_fn)\n",
    "\n",
    "# train_dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=my_collate_fn)\n",
    "\n",
    "ner_model = NERNeuralNetwork(embd_dim=256, heads=8, forward_expansion=3, dropout=0.5, vocab_size=word_vocab.n_words, max_sent_len=word_vocab.max_sent_len, total_tags=tag_vocab.n_tags)\n",
    "\n",
    "train(dataloader=train_loader, ner_model=ner_model, print_every=10, learning_rate=0.001, n_epochs=50)\n",
    "\n",
    "print('Evaluating the model.')\n",
    "\n",
    "ner_model.load_state_dict(torch.load(\"./ner_model.pth\"))\n",
    "\n",
    "val_loss, val_acc = evaluate(ner_model=ner_model, dataloader=test_loader)\n",
    "print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
